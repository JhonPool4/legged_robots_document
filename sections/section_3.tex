\section{Related work}

Currently, there are several methods to develop drivers for robots with legs. The classic ones use previous knowledge about the robot, such as its dynamic model or information obtained from simulations involving them. For example, the ANYmal robot from ETH uses the inverted pendulum model to generate joint torques, contact forces and motion \cite{anymal}; the MIT Cheetah uses a model predictive control to plan desired contact forces \cite{cheetah}.

However, in certain cases, it can be difficult to obtain prior knowledge of the robot through dynamic models and simulations. In this context, a feasible approach to controller design is to use deep learning, more specifically, deep reinforcement learning. Furthermore, another advantage that the use of machine learning provides is that it does not accumulate data generated by the robot, it only uses what is currently being processed, in addition to the fact that the robot can learn to march on its own \cite{haarnoja2018learning}.

Controller development methods using deep reinforcement learning may vary depending on the chosen algorithm. The University of California, for example, used a learning algorithm based on maximum entropy reinforcement learning, training a Minitaur robot directly, without having to train a simulation beforehand \cite{haarnoja2018learning}.

In this work, a class of deep reinforcement learning algorithms — Proximal Policy Optimization (PPO) — will be used to train a simulation of a biped developed in MuJoCo, a simulator for multi-body dynamics.
