
\section{Introduction}
Mobile robots have gained more notoriety over the years, as the development of new technologies allows the implementation of these robotic systems in different areas: from agriculture to the exploration of satellites and planets \textbf{[reference]}. They are classified into wheeled, tracked, undulating, aerial and legged \textbf{[reference]}. In this way, they can use abilities already known in fixed robots (e.g. robotic manipulators), such as applying forces greater than those that a human being can perform with high precision, but with a mobile base. For this reason, they are able to perform repetitive, dangerous or demanding work that requires strength and precision, precision beyond what a human being is capable of, in distant places, without an individual needing to take the robot to the task site.

In this context, among the different types of mobile robots, perhaps the best known are wheeled robots due to the efforts to develop fully autonomous cars, and also because of rovers that currently explore Mars \textbf{[reference]}. However, using these robots in very rough terrain can bring some disadvantages, such as tipping due to some instability that the robot may have due to the unevenness of the ground. Factors that can interfere with the proper functioning of wheeled robots are, for example, holes, steps and steep slopes. In these cases, opting for legged robots is a good option, as they can get around these problems: legs can dodge holes, walk on steps and march up steep climbs.

The design of controllers that can keep these robots stable and functioning properly, becomes more difficult, since it is necessary to control numerous actuators, at the same time, in each of the legs, so that the robot can march correctly \textbf{[reference]}. Thus, there are different approaches to developing legged robot controllers\textbf{[reference]}. Some of them involve the use of previous robot information, such as dynamic models and computer simulations. However, it is not always so easy to obtain this data, which can complicate the design of the controller. Other approaches involve the use of machine learning, eliminating the need to model the dynamic behavior of the robot, as it will learn from its own mistakes \cite{kormushev2013reinforcement}.

Deep reinforcement learning (DRL) is a machine training method to teach an agent to take a good sequence of decisions (actions) \cite{sutton2018reinforcement}. DRL works with the simple idea of giving positive (reward) or negative (punishment) reinforcement to the agent's behavior. In this way, the agent learns to make decisions that guarantee the highest amount of reward. Due to its simple operating mechanism, DRL can be adapted to solve complex optimization problems in the robotics area \cite{kober2013reinforcement}. For this reason, recent works use DRL with proximal policy optimization (PPO) algorithm to control the waking of Cassie, ABL-BI and Robotis-op3 robots \cite{xie2018feedback}, \cite{beranek2021behavior}, \cite{jiang2020motion}. In these three works, the agent learned strategies to maintain the body's balance despite external disturbances of force and uneven terrain. 

Currently, there are several methods to develop drivers for robots with legs. The classic ones use previous knowledge about the robot, such as its dynamic model or information obtained from simulations involving them. For example, the ANYmal robot from ETH uses the inverted pendulum model to generate joint torques, contact forces and motion \cite{anymal}; the MIT Cheetah uses a model predictive control to plan desired contact forces \cite{cheetah}.

However, in certain cases, it can be difficult to obtain prior knowledge of the robot through dynamic models and simulations. In this context, a feasible approach to controller design is to use deep learning, more specifically, deep reinforcement learning. Furthermore, another advantage that the use of machine learning provides is that it does not accumulate data generated by the robot, it only uses what is currently being processed, in addition to the fact that the robot can learn to march on its own \cite{haarnoja2018learning}.

Controller development methods using deep reinforcement learning may vary depending on the chosen algorithm. The University of California, for example, used a learning algorithm based on maximum entropy reinforcement learning, training a Minitaur robot directly, without having to train a simulation beforehand \cite{haarnoja2018learning}.

The proximal policy optimization algorithm formulate a objective function related with the reward and then use the gradient of the objective function to seek the optimal policy \cite{schulman2017proximal}. PPO is one of the best algorithms for reinforcement learning applications due to its simplicity and high efficiency~\cite{schulman2017proximal}. This algorithm has been successfully implemented to solve problems in bipedal walking \cite{melo2019learning}, games \cite{kristensen2020strategies} and unmanned aerial vehicles \cite{bohn2019deep}.


This work perform the walking of a bipedal robot using deep reinforcement learning with proximal policy optimization. For this purpose, the MuJoCo simulator is used to compute the dynamics of the bipedal robot. Likewise, OpenAI gym toolkit is used to compute the reinforcement learning equations.

% first draft
The document is structured as follows. First, section II explain reinforcement learning method. Second, section III describes the policy proximal optimization algorithm. Third, section IV indicates the open-source libraries and simulator used in this work. Section V presents the results of the work. Finally, section VI describes conclusions 


