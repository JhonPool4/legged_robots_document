@article{lee2008importance,
	title={The importance of walking to public health},
	author={Lee, I-Min and Buchner, David M},
	journal={Medicine \& Science in Sports \& Exercise},
	volume={40},
	number={7},
	pages={S512--S518},
	year={2008},
	publisher={LWW}
}

@INPROCEEDINGS{anymal,
  author={Hutter, Marco and Gehring, Christian and Jud, Dominic and Lauber, Andreas and Bellicoso, C. Dario and Tsounis, Vassilios and Hwangbo, Jemin and Bodie, Karen and Fankhauser, Peter and Bloesch, Michael and Diethelm, Remo and Bachmann, Samuel and Melzer, Amir and Hoepflinger, Mark},
  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={ANYmal - a highly mobile and dynamic quadrupedal robot}, 
  year={2016},
  volume={},
  number={},
  pages={38-44},
  doi={10.1109/IROS.2016.7758092}
}

@INPROCEEDINGS{cheetah,
  author={Bledt, Gerardo and Powell, Matthew J. and Katz, Benjamin and Di Carlo, Jared and Wensing, Patrick M. and Kim, Sangbae},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={MIT Cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot}, 
  year={2018},
  volume={},
  number={},
  pages={2245-2252},
  doi={10.1109/IROS.2018.8593885}}

@article{haarnoja2018learning,
  title={Learning to walk via deep reinforcement learning},
  author={Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.11103},
  year={2018}
}



% bipedal walking using deep reinforcement learning
@inproceedings{xie2018feedback,
	title={Feedback control for cassie with deep reinforcement learning},
	author={Xie, Zhaoming and Berseth, Glen and Clary, Patrick and Hurst, Jonathan and van de Panne, Michiel},
	booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	pages={1241--1246},
	year={2018},
	organization={IEEE}
}

@article{beranek2021behavior,
	title={A Behavior-Based Reinforcement Learning Approach to Control Walking Bipedal Robots Under Unknown Disturbances},
	author={Beranek, Richard and Karimi, Masoud and Ahmadi, Mojtaba},
	journal={IEEE/ASME Transactions on Mechatronics},
	year={2021},
	publisher={IEEE}
}

@inproceedings{vaghei2014actor,
	title={Actor-critic neural network reinforcement learning for walking control of a 5-link bipedal robot},
	author={Vaghei, Yasaman and Ghanbari, Ahmad and Noorani, Sayyed Mohammad Reza Sayyed},
	booktitle={2014 Second RSI/ISM International Conference on Robotics and Mechatronics (ICRoM)},
	pages={773--778},
	year={2014},
	organization={IEEE}
}

@inproceedings{jiang2020motion,
	title={Motion Sequence Learning for Robot Walking Based on Pose optimization},
	author={Jiang, Yancao and Zhang, Weiyi and Farrukh, Fasih Ud Din and Xie, Xiang and Zhang, Chun},
	booktitle={2020 IEEE International Conference on Mechatronics and Automation (ICMA)},
	pages={1877--1882},
	year={2020},
	organization={IEEE}
}

% reinforcement learning theory
@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018},
	publisher={MIT press}
}

% reinforcement learning in robotic applications
@article{kober2013reinforcement,
	title={Reinforcement learning in robotics: A survey},
	author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
	journal={The International Journal of Robotics Research},
	volume={32},
	number={11},
	pages={1238--1274},
	year={2013},
	publisher={SAGE Publications Sage UK: London, England}
}
@article{kormushev2013reinforcement,
	title={Reinforcement learning in robotics: Applications and real-world challenges},
	author={Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G},
	journal={Robotics},
	volume={2},
	number={3},
	pages={122--148},
	year={2013},
	publisher={Multidisciplinary Digital Publishing Institute}
}

% proximal policy optimization (PPO): formulation
@article{schulman2017proximal,
	title={Proximal policy optimization algorithms},
	author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	journal={arXiv preprint arXiv:1707.06347},
	year={2017}
}

% proximal policy optimization: applications
@inproceedings{melo2019learning,
	title={Learning humanoid robot running skills through proximal policy optimization},
	author={Melo, Luckeciano Carvalho and M{\'a}ximo, Marcos Ricardo Omena Albuquerque},
	booktitle={2019 Latin american robotics symposium (LARS), 2019 Brazilian symposium on robotics (SBR) and 2019 workshop on robotics in education (WRE)},
	pages={37--42},
	year={2019},
	organization={IEEE}
}

@inproceedings{bohn2019deep,
	title={Deep reinforcement learning attitude control of fixed-wing uavs using proximal policy optimization},
	author={B{\o}hn, Eivind and Coates, Erlend M and Moe, Signe and Johansen, Tor Ame},
	booktitle={2019 International Conference on Unmanned Aircraft Systems (ICUAS)},
	pages={523--533},
	year={2019},
	organization={IEEE}
}

@inproceedings{kristensen2020strategies,
	title={Strategies for using proximal policy optimization in mobile puzzle games},
	author={Kristensen, Jeppe Theiss and Burelli, Paolo},
	booktitle={International conference on the foundations of digital games},
	pages={1--10},
	year={2020}
}

% sthocastic policies
@inproceedings{tedrake2004stochastic,
	title={Stochastic policy gradient reinforcement learning on a simple 3D biped},
	author={Tedrake, Russ and Zhang, Teresa Weirui and Seung, H Sebastian},
	booktitle={2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566)},
	volume={3},
	pages={2849--2854},
	year={2004},
	organization={IEEE}
}

% introduction
@article{rong2012configuration,
  title={Configuration analysis and structure parameter design of six-leg agricultural robot with parallel-leg mechanisms},
  author={Rong, Yu and Jin, Zhenlin and Cui, Bingyan},
  journal={Transactions of the Chinese Society of Agricultural Engineering},
  volume={28},
  number={15},
  pages={9--14},
  year={2012},
  publisher={Editorial Office of Transactions of the Chinese Society of Agricultural~â€¦}
}

@inproceedings{arm2019spacebok,
	title={Spacebok: A dynamic legged robot for space exploration},
	author={Arm, Philip and Zenkl, Radek and Barton, Patrick and Beglinger, Lars and Dietsche, Alex and Ferrazzini, Luca and Hampp, Elias and Hinder, Jan and Huber, Camille and Schaufelberger, David and others},
	booktitle={2019 international conference on robotics and automation (ICRA)},
	pages={6288--6294},
	year={2019},
	organization={IEEE}
}

@article{maurette2003mars,
	title={Mars rover autonomous navigation},
	author={Maurette, Michel},
	journal={Autonomous Robots},
	volume={14},
	number={2},
	pages={199--208},
	year={2003},
	publisher={Springer}
}

@inproceedings{kalakrishnan2010fast,
	title={Fast, robust quadruped locomotion over challenging terrain},
	author={Kalakrishnan, Mrinal and Buchli, Jonas and Pastor, Peter and Mistry, Michael and Schaal, Stefan},
	booktitle={2010 IEEE International Conference on Robotics and Automation},
	pages={2665--2670},
	year={2010},
	organization={IEEE}
}

@article{bellicoso2018dynamic,
	title={Dynamic locomotion through online nonlinear motion optimization for quadrupedal robots},
	author={Bellicoso, C Dario and Jenelten, Fabian and Gehring, Christian and Hutter, Marco},
	journal={IEEE Robotics and Automation Letters},
	volume={3},
	number={3},
	pages={2261--2268},
	year={2018},
	publisher={IEEE}
}

@article{heess2016learning,
	title={Learning and transfer of modulated locomotor controllers},
	author={Heess, Nicolas and Wayne, Greg and Tassa, Yuval and Lillicrap, Timothy and Riedmiller, Martin and Silver, David},
	journal={arXiv preprint arXiv:1610.05182},
	year={2016}
}

@article{peng2017deeploco,
	title={Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning},
	author={Peng, Xue Bin and Berseth, Glen and Yin, KangKang and Van De Panne, Michiel},
	journal={ACM Transactions on Graphics (TOG)},
	volume={36},
	number={4},
	pages={1--13},
	year={2017},
	publisher={ACM New York, NY, USA}
	
@article{siciliano2009robotics,
	title={Robotics modeling, planning and control Springer},
	author={Siciliano, B and Sciavicco, L and Villani, L and Oriolo, G},
	journal={Verlag, London},
	year={2009}
	}
	
% deep reinforcement learning
@inproceedings{henderson2018deep,
	title={Deep reinforcement learning that matters},
	author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	booktitle={Proceedings of the AAAI conference on artificial intelligence},
	volume={32},
	number={1},
	year={2018}
}

@article{li2017deep,
	title={Deep reinforcement learning: An overview},
	author={Li, Yuxi},
	journal={arXiv preprint arXiv:1701.07274},
	year={2017}
}

% policy gradient methods
@article{thomas2017policy,
	title={Policy gradient methods for reinforcement learning with function approximation and action-dependent baselines},
	author={Thomas, Philip S and Brunskill, Emma},
	journal={arXiv preprint arXiv:1706.06643},
	year={2017}
}

% mujoco
@inproceedings{todorov2012mujoco,
title={Mujoco: A physics engine for model-based control},
author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
pages={5026--5033},
year={2012},
organization={IEEE}
}

